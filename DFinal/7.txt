import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk import pos_tag

nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger_eng')

document = "Text analytics is an exciting field. It helps computers understand human language!"

#Tokenization
tokens = word_tokenize(document)
print("Tokens:", tokens)

#POS Tagging
pos_tags = pos_tag(tokens)
print("\nPOS Tags:", pos_tags)

#Stop Words Removal
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
print("\nAfter Stopwords Removal:", filtered_tokens)

#Stemming
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]
print("\nAfter Stemming:", stemmed_tokens)

#Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]
print("\nAfter Lemmatization:", lemmatized_tokens)


#part2
from sklearn.feature_extraction.text import TfidfVectorizer
 corpus = [
    "Sachin was the GOAT of the previous generation",
    "Virat is the GOAT of the this generation",
    "Shubman will be the GOAT of the next generation"
 ]

vectorizer = TfidfVectorizer()
matrix = vectorizer.fit(corpus)
matrix.vocabulary_

tfidf_matrix = vectorizer.transform(corpus)
print(tfidf_matrix)

print(vectorizer.get_feature_names_out())